{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e82e5d-5c4c-4149-ab00-ea818c95b713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0996ab-2d6b-4eac-b4e5-d75dcda28c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random # random class\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm # loop progress\n",
    "\n",
    "from fredapi import Fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce65761-29f0-4c01-90ad-a093eea31ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your Eikon API app key\n",
    "import eikon as ek\n",
    "ek.set_app_key(\"d9b8f435f4694441a667806b4245931101403e3e\") # this will not work for you if you dont have an refinitiv account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c04ed-527f-4614-b0e8-14550c7fb71d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get sp500 constituents from eikon (ignore, get it from file -> next couple of cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed3195-e492-4842-9446-06308bb56c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download data\n",
    "def GetStockData(ticker, start, end, interval):\n",
    "        temp_data = ek.get_timeseries([ticker], \n",
    "                                      start_date = start, \n",
    "                                      end_date = end, \n",
    "                                      interval=interval)\n",
    "        return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1677e2-5b4f-4409-90b7-4f9dc84b036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# create a list of dates\n",
    "start_date = datetime.date(2023, 4, 1)\n",
    "end_date = start_date #datetime.date.today()\n",
    "\n",
    "date_list = []\n",
    "for year in range(start_date.year, end_date.year + 1):\n",
    "    for month in range(1, 13):\n",
    "        if year == start_date.year and month < start_date.month:\n",
    "            continue\n",
    "        elif year == end_date.year and month > end_date.month:\n",
    "            break\n",
    "        else:\n",
    "            date_list.append(datetime.date(year, month, 1).strftime('%Y-%m-%d'))\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f349ad4-981d-4f89-95d5-538db7fc2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get constituents for specific dates\n",
    "Ticker_ISIN_GICSSector_dict = {}\n",
    "\n",
    "for date in tqdm(date_list):\n",
    "    temp_data = ek.get_data('.SPX', ['TR.IndexConstituentRIC', 'TR.IndexConstituentName'], {'SDate':date})[0]\n",
    "    riclist = temp_data['Constituent RIC'].tolist()\n",
    "    temp_data_2 = ek.get_data(riclist, ['TR.ISIN', 'TR.GICSSector'])\n",
    "    Ticker_ISIN_GICSSector_dict[date] = temp_data_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b670a53-f499-41ed-80df-78c1029dc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique lists\n",
    "isin_set = set()\n",
    "ticker_set = set()\n",
    "for date in Ticker_ISIN_GICSSector_dict:\n",
    "    if isinstance(Ticker_ISIN_GICSSector_dict[date], tuple):\n",
    "        isin_data = Ticker_ISIN_GICSSector_dict[date][0][['ISIN']]\n",
    "        ticker_data = Ticker_ISIN_GICSSector_dict[date][0][['Instrument']]\n",
    "    else:\n",
    "        isin_data = Ticker_ISIN_GICSSector_dict[date][['ISIN']]\n",
    "        ticker_data = Ticker_ISIN_GICSSector_dict[date][['Instrument']]\n",
    "    isin_set.update(isin_data['ISIN'].tolist())\n",
    "    ticker_set.update(ticker_data['Instrument'].tolist())\n",
    "\n",
    "isin_df = pd.DataFrame({'ISIN': list(isin_set)})\n",
    "isin_list = list(isin_set)\n",
    "ticker_list = list(ticker_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58dfe5-1f5b-4113-9c01-b5f7dbe9dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with all close prices\n",
    "\n",
    "# define the parameters for getting the stock data\n",
    "start_date = \"2019-12-31\"\n",
    "end_date = \"2023-03-31\"\n",
    "interval = \"daily\"\n",
    "\n",
    "# create an empty dictionary to store the resulting dataframes\n",
    "result_dict = {}\n",
    "# loop through the ticker list and call the function for each ticker\n",
    "for ticker in ticker_list:\n",
    "    try:\n",
    "        result_dict[ticker] = GetStockData(ticker, start_date, end_date, interval)\n",
    "        print(ticker, \"downloaded successfully\")\n",
    "    except:\n",
    "        print(ticker, \"not successful\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6476631-9530-4416-a082-8f35915b46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all close prices to one dataframe\n",
    "df_list = []\n",
    "\n",
    "# Iterate over the dictionary items and reset the index of each DataFrame\n",
    "for key, value in result_dict.items():\n",
    "    value = value.reset_index()\n",
    "    # Select the 'Date' and 'CLOSE' columns\n",
    "    value = value[['Date', 'CLOSE']]\n",
    "    # Rename the 'CLOSE' column to the key\n",
    "    value = value.rename(columns={'CLOSE': key})\n",
    "    # Set the 'Date' column as the index\n",
    "    value = value.set_index('Date')\n",
    "    # Add the DataFrame to the list\n",
    "    df_list.append(value)\n",
    "\n",
    "# Join the DataFrames on the index with an outer join\n",
    "result_df = df_list[0]\n",
    "for i in range(1, len(df_list)):\n",
    "    result_df = result_df.join(df_list[i], how='outer')\n",
    "\n",
    "# get rid of incomplete columns\n",
    "result_df_clean = result_df.loc[:, result_df.count() == max(result_df.describe().T[\"count\"])]\n",
    "nans_count = result_df_clean.isna().sum().sum()\n",
    "print(\"Number of NaN values in result_df:\", nans_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120487ec-f920-47e1-b95d-5dc3f0273527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 5 day returns\n",
    "weekly_returns = result_df_clean.pct_change().rolling(window=5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f7075-ea6a-4c7b-b5e6-13fd52bdb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the file path to where you want to save the DataFrame\n",
    "desktop_path = '~/Desktop/ML2_assignment_5_project/'\n",
    "file_name = 'clean_data_sp500_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "file_path = desktop_path + file_name\n",
    "# save the DataFrame to a CSV file with a timestamp index and current date and time in the filename\n",
    "result_df_clean.round(2).to_csv(file_path, index_label='timestamp', date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c5d37-0774-4eda-9065-ead0de68266f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get sp500 constituents from file (use this, the files are hard copied in our Git repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def42d52-8d67-4f35-a36b-0af84c6477ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from desktop\n",
    "path = r\"~/Desktop/ML2_assignment_5_project/clean_data_sp500_20230421_150246.csv\"\n",
    "result_df_clean = pd.read_csv(path)\n",
    "result_df_clean = result_df_clean.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b46fb-d9b0-4123-9fe6-be6fa068ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 5 day returns\n",
    "weekly_returns = result_df_clean.pct_change().rolling(window=5).sum()\n",
    "\n",
    "# define the type of change should be used\n",
    "target = weekly_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63e1d-be1d-4ce0-b9b5-90d468640e51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get GICS Sectors, again, eikon code cor completeness but use the files in the repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65f81b-0845-4513-b90e-4367736a6fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eikon (ignore)\n",
    "\n",
    "sector_classes_code, err = ek.get_data(target.columns.tolist(), \n",
    "                                       ['TR.GICSSectorCode', 'TR.GICSIndustryGroupCode', \n",
    "                                        'TR.GICSIndustryCode', 'TR.GICSSubIndustryCode'])\n",
    "sector_classes, err = ek.get_data(target.columns.tolist(), \n",
    "                                  ['TR.GICSSector', 'TR.GICSIndustryGroup', \n",
    "                                   'TR.GICSIndustry', 'TR.GICSSubIndustry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcb904f-aabe-467d-96d4-f60102c2f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"~/Desktop/ML2_assignment_5_project/sector_classes_code_20230503_005638.csv\" # update\n",
    "sector_classes_code = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db08193-a01f-443a-b941-6f9d72b2a0fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get more idiosyncratic features (again Eikon, files in the repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78ef5fc-0f59-4879-9190-67a6f803a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"TR.ROAActValue\",\n",
    "    \"TR.PriceToSalesPerShare\",\n",
    "    \"TR.TotalDebtToEV\",\n",
    "    \"TR.PriceToBVPerShare\", \n",
    "    \"TR.CompanyMarketCapitalization\", \n",
    "    \"TR.PriceNetChg30D\", \n",
    "    \"TR.AvgDailyValTraded30D\", \n",
    "    \"TR.SharpeRatioWkly2Y\", \n",
    "    \"TR.Volatility260D\", \n",
    "    \"TR.BetaWklyUp2Y\", \n",
    "    \"TR.BetaWklyDown2Y\", \n",
    "    \"TR.RSISimple14D\", \n",
    "    \"TR.ShortInterestPCT\", \n",
    "    \"TR.ShortInterestDTC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ed27c-df14-4a91-8a95-f7fcfdd34723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, e = ek.get_data('0#.SPX', fields, parameters={'SDate': '2022-12-31', 'EDate':'2022-12-31'})\n",
    "df2 = df2.set_index(df2.columns[0])\n",
    "df2['Return On Assets - Actual'].fillna(df2['Return On Assets - Actual'].median(), inplace=True)\n",
    "df2_without_nan = df2.dropna()\n",
    "#rename a few columns\n",
    "df2_without_nan = df2_without_nan.rename(columns={'Price To Sales Per Share (Daily Time Series Ratio)': 'Price To Sales Per Share'})\n",
    "df2_without_nan = df2_without_nan.rename(columns={'Total Debt To Enterprise Value (Daily Time Series Ratio)': 'Total Debt To Enterprise Value'})\n",
    "df2_without_nan = df2_without_nan.rename(columns={'Price To Book Value Per Share (Daily Time Series Ratio)': 'Price To Book Value Per Share'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b7f7e-cb11-4e55-8cd1-3c4039a3a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f897c-0df1-4ba4-bf4f-a2dae25eea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "idio_feats = df2_without_nan.astype(float)\n",
    "\n",
    "#ln transformation with negatives\n",
    "ln_transform_list = [\"Price To Sales Per Share\",\n",
    "                     \"Total Debt To Enterprise Value\",\n",
    "                     \"Price To Book Value Per Share\",\n",
    "                     \"Company Market Capitalization\",\n",
    "                     \"Average Daily Value Traded - 30 Days\"]\n",
    "for i in ln_transform_list:\n",
    "    idio_feats.loc[:, i] = np.where(idio_feats[i] <= 0, 0, np.log(1+idio_feats[i]))\n",
    "\n",
    "# price change transform\n",
    "idio_feats.loc[:, \"Trailing 30-day Price Net Change\"] = np.clip(0.01*idio_feats[\"Trailing 30-day Price Net Change\"], -0.5, 0.5)\n",
    "\n",
    "# new feature: convexity \n",
    "idio_feats.loc[:, \"Convexity\"] = (idio_feats[\"Weekly Beta Up - 2 Year\"] / idio_feats[\"Weekly Beta Down - 2 Year\"])\n",
    "idio_feats = idio_feats.drop(['Weekly Beta Up - 2 Year', 'Weekly Beta Down - 2 Year'], axis=1)\n",
    "idio_feats.loc[:, \"Convexity\"] = sigmoid(idio_feats.loc[:, \"Convexity\"])\n",
    "# short interest features\n",
    "idio_feats.loc[:, \"Days To Cover\"] = np.log(1+idio_feats[\"Days To Cover\"])\n",
    "idio_feats.loc[:, \"Short Interest Pct\"] = np.log(idio_feats[\"Short Interest Pct\"])\n",
    "\n",
    "# profitability\n",
    "idio_feats.loc[:, \"Return On Assets - Actual\"] = np.clip(np.log(1+idio_feats[\"Return On Assets - Actual\"]*0.01), -0.1,1)\n",
    "\n",
    "# vola\n",
    "idio_feats.loc[:, \"Volatility - 260 days\"] = np.log(idio_feats[\"Volatility - 260 days\"])\n",
    "\n",
    "# standardize each row\n",
    "idio_feats = idio_feats.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ba949-bef1-428a-b6d9-2c4a4400a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all tickers alphabetically\n",
    "# idiosyncratic_feature_stocks = list(idio_feats.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e3b68-2e1d-4ca5-b2d9-02bd8d9a9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idiosyncratic feature plot\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = idio_feats\n",
    "\n",
    "# Plot time series on diagonal\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "\n",
    "# Add correlation coefficients to lower triangle with colors\n",
    "corr_matrix = df.corr()\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "for i, j in zip(*np.tril_indices_from(g.axes)):\n",
    "    if i != j:\n",
    "        coef = corr_matrix.iloc[i, j]\n",
    "        g.axes[i, j].annotate(f\"{coef:.2f}\", (0.1, 0.9), xycoords='axes fraction', ha='left', va='center', color='w', fontsize=10, bbox=dict(boxstyle=\"round\", facecolor=cmap(coef/2 + 0.5), alpha=1))\n",
    "\n",
    "g.map_upper(sns.scatterplot)\n",
    "#g.map_lower(sns.histplot, color='steelblue', bins=20, edgecolor='white')\n",
    "g.map_lower(sns.kdeplot, shade=True)\n",
    "g.map_diag(sns.kdeplot, shade=True)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1347b-c5c4-4180-84ee-2f2b92c982e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the file path to where you want to save the DataFrame\n",
    "desktop_path = '~/Desktop/ML2_assignment_5_project/'\n",
    "file_name = 'idi_features{}.csv'\n",
    "file_path = desktop_path + file_name\n",
    "# save the DataFrame to a CSV file with a timestamp index and current date and time in the filename\n",
    "df2_without_nan.round(5).to_csv(file_path, index_label='timestamp', date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d88ab3-ddec-4246-a80f-fa5736526eda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get macro variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d644d74-7dc5-44f7-8b7b-9f0a7d3250d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import FRED data\n",
    "fred = Fred(api_key = \"4a017ca39a1f96774f9587e5956bfd6b\") # Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451fba3-d5dc-4f75-a917-56241da9f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a dataframe with the variables\n",
    "def create_table(series, column_name):\n",
    "    df = series.to_frame()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"index\": \"date\", 0: column_name})\n",
    "    return df.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b6106-3b5b-4e32-84e2-d6edc041d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables: \"ticker\": \"clear_name\"\n",
    "x_factors_dict = {\n",
    "    \"SP500\": \"S&P500\", #risk\n",
    "    \"VIXCLS\": \"VIX\", #risk\n",
    "    \n",
    "    \"DGS10\": \"_10_y_UST\", #rate\n",
    "    \"T10Y2Y\": \"_2s10s_UST\", #rate\n",
    "    \"T10YIE\": \"_10_y_BE\", #rate\n",
    "    \n",
    "    \"DEXUSEU\": \"USD_to_EUR\", #fx\n",
    "    \"DEXUSUK\": \"USD_to_GBP\", #fx\n",
    "    \"DEXJPUS\": \"JPY_to_USD\", #fx\n",
    "    \"DEXCHUS\": \"CNH_to_USD\", #fx\n",
    "    \"DEXMXUS\": \"MXN_to_USD\", #fx\n",
    "    \"DEXCAUS\": \"CAD_to_USD\", #fx\n",
    "    \n",
    "    \"DCOILBRENTEU\": \"Brent_Crude\", #cmdty\n",
    "    \n",
    "    \"WILLLRGCAP\": \"Large\", #factor\n",
    "    \"WILLSMLCAP\": \"Small\", #factor\n",
    "    \"WILLLRGCAPVAL\": \"Value\", #factor\n",
    "    \"WILLLRGCAPGR\": \"Growth\" #factor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ebe03-4b4d-4437-bcf4-4a00374e9c2d",
   "metadata": {},
   "source": [
    "why these variables?\n",
    "\n",
    "#risk: overall equity market exposure\n",
    "\n",
    "#rate: overall (real)rate and inflation exposure\n",
    "\n",
    "#fx: mayjor currencies and neighbours currencies (some US stocks might be dependend on other regions)\n",
    "\n",
    "#factor: small cap premium and value premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89a0ce-bcd3-486b-b469-1c852c462d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will download the data. for each variable the total history will be downloaded! dont know why the date does not work...\n",
    "dfs = []\n",
    "for fred_series_id, column_name in x_factors_dict.items():\n",
    "    series = fred.get_series(fred_series_id)\n",
    "    df = create_table(series, column_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=1)\n",
    "\n",
    "# we fill NaN with the previous value\n",
    "df_filled = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e277e3-4878-4879-9a1a-47522acb9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a little feature engineering (takes a few seconds)\n",
    "df_filled[\"ValueOverGrowth\"] = df_filled[\"Value\"]/df_filled[\"Growth\"]\n",
    "df_filled[\"SmallOverLarge\"] = df_filled[\"Small\"]/df_filled[\"Large\"]\n",
    "df_filled[\"_10_y_UST_real\"] = df_filled[\"_10_y_UST\"]-df_filled[\"_10_y_BE\"]\n",
    "df_raw_features = df_filled.drop(['Large', 'Small', 'Value', 'Growth'], axis=1)\n",
    "\n",
    "new_order = [\"S&P500\", \"VIX\", \"ValueOverGrowth\", \"SmallOverLarge\", \"USD_to_EUR\", \n",
    "             \"USD_to_GBP\", \"JPY_to_USD\", \"CNH_to_USD\", \"MXN_to_USD\", \"CAD_to_USD\",\n",
    "             \"_10_y_UST\", \"_10_y_UST_real\", \"_10_y_BE\", \"_2s10s_UST\", \"Brent_Crude\"]\n",
    "\n",
    "df_raw_features = df_raw_features[new_order]\n",
    "df_raw_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3d51f-4bcd-47e3-8df1-adeb1c39653f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visualizing all raw features (this takes a few seconds)\n",
    "as_of = '2019-12-31'\n",
    "df = df_raw_features[df_raw_features.index >= as_of]\n",
    "\n",
    "# define the number of rows and columns for the plot grid\n",
    "nrows = 3\n",
    "ncols = 5\n",
    "\n",
    "# create a new figure and axes\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(25, 10))\n",
    "\n",
    "# flatten the axes array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# iterate over the columns and create subplots\n",
    "for i, col in enumerate(df.columns):\n",
    "    ax = axs[i]\n",
    "    sns.lineplot(x=df.index, y=col, data=df, ax=ax)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis='x', rotation=45)  # set the x-tick label rotation to 45 degrees\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "# hide the unused subplots\n",
    "for i in range(len(df.columns), nrows*ncols):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "# adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "\n",
    "# add a title to the plot grid\n",
    "fig.suptitle('Raw Macro Variables as of ' + as_of, fontsize=20, fontweight='bold')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ce2e9-1c92-4a98-a3bd-79d89b2bcd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and slice the dateframe to the period we want\n",
    "# start_date = \"2019-12-31\" end_date = \"2023-03-31\" is used in the initial data sourcing from eikon\n",
    "\n",
    "start_date = \"2019-12-31\"\n",
    "end_date = \"2023-03-31\"\n",
    "\n",
    "df_raw_features_slice = df_raw_features.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596da1fa-4f4e-4229-bb5c-4389511f13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the changes\n",
    "\n",
    "abs_changes = [\"VIX\", \"_10_y_UST\", \"_10_y_UST_real\", \"_10_y_BE\", \"_2s10s_UST\"]\n",
    "rel_changes = [\"S&P500\", \"ValueOverGrowth\", \"SmallOverLarge\", \"USD_to_EUR\", \"USD_to_GBP\",\n",
    "               \"JPY_to_USD\", \"CNH_to_USD\", \"MXN_to_USD\", \"CAD_to_USD\",\"Brent_Crude\"]\n",
    "\n",
    "df_raw_features_slice_chg = df_raw_features_slice.copy()\n",
    "df_raw_features_slice_chg.loc[:, rel_changes] = df_raw_features_slice[rel_changes].pct_change().rolling(window=5).sum()\n",
    "df_raw_features_slice_chg.loc[:, abs_changes] = df_raw_features_slice[abs_changes].diff().rolling(window=5).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12223dbe-cc82-4f95-9bca-1fe3169a8621",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting the macrovariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a04819-65ca-4e47-9ce0-40a9604107e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature plot changes\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = df_raw_features_slice_chg.iloc[5:]\n",
    "\n",
    "# Plot time series on diagonal\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "\n",
    "# Add correlation coefficients to lower triangle with colors\n",
    "corr_matrix = df.corr()\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "for i, j in zip(*np.tril_indices_from(g.axes)):\n",
    "    if i != j:\n",
    "        coef = corr_matrix.iloc[i, j]\n",
    "        g.axes[i, j].annotate(f\"{coef:.2f}\", (0.1, 0.9), xycoords='axes fraction', ha='left', va='center', color='w', fontsize=10, bbox=dict(boxstyle=\"round\", facecolor=cmap(coef/2 + 0.5), alpha=1))\n",
    "\n",
    "g.map_upper(sns.scatterplot)\n",
    "#g.map_lower(sns.histplot, color='steelblue', bins=20, edgecolor='white')\n",
    "g.map_lower(sns.kdeplot, shade=True)\n",
    "g.map_diag(sns.kdeplot, shade=True)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69756fed-cfa8-4955-a60a-da77afd06fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature plot changes\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = df_raw_features_slice_chgLT.iloc[20:]\n",
    "\n",
    "# Plot time series on diagonal\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "\n",
    "# Add correlation coefficients to lower triangle with colors\n",
    "corr_matrix = df.corr()\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "for i, j in zip(*np.tril_indices_from(g.axes)):\n",
    "    if i != j:\n",
    "        coef = corr_matrix.iloc[i, j]\n",
    "        g.axes[i, j].annotate(f\"{coef:.2f}\", (0.1, 0.9), xycoords='axes fraction', ha='left', va='center', color='w', fontsize=10, bbox=dict(boxstyle=\"round\", facecolor=cmap(coef/2 + 0.5), alpha=1))\n",
    "\n",
    "g.map_upper(sns.scatterplot, cmap=cmap)\n",
    "g.map_lower(sns.kdeplot, shade=True)\n",
    "g.map_diag(sns.kdeplot, shade=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3e062-990e-4795-ab0d-1c7a92a70a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature plot \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "df = df_raw_features_slice\n",
    "\n",
    "# Plot time series on diagonal\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "\n",
    "# Add correlation coefficients to lower triangle with colors\n",
    "corr_matrix = df.corr()\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "for i, j in zip(*np.tril_indices_from(g.axes)):\n",
    "    if i != j:\n",
    "        coef = corr_matrix.iloc[i, j]\n",
    "        g.axes[i, j].annotate(f\"{coef:.2f}\", (0.1, 0.9), xycoords='axes fraction', ha='left', va='center', color='w', fontsize=10, bbox=dict(boxstyle=\"round\", facecolor=cmap(coef/2 + 0.5), alpha=1))\n",
    "\n",
    "g.map_upper(sns.scatterplot, cmap=cmap)\n",
    "g.map_lower(sns.kdeplot, shade=True)\n",
    "g.map_diag(sns.kdeplot, shade=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15388b6-5c23-4da9-8698-50dc850ab9c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## merging all raw data to one big dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9118cf6-4aa5-4a40-ba00-2e2b3b7db949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n features\n",
    "nfeat = df_raw_features_slice_chg.shape[1]\n",
    "\n",
    "# Rename the index to date\n",
    "target = target.rename_axis(\"date\")\n",
    "df_raw_features_slice_chg = df_raw_features_slice_chg.rename_axis(\"date\")\n",
    "\n",
    "# Convert the index to datetime\n",
    "target.index = pd.to_datetime(target.index)\n",
    "df_raw_features_slice_chg.index = pd.to_datetime(df_raw_features_slice_chg.index)\n",
    "\n",
    "# Merge using left join, based on their index\n",
    "total_raw_data = target.merge(df_raw_features_slice_chg, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Detrend stocks by substracting s&p50\n",
    "detrende_stocks = (total_raw_data.iloc[:, :-nfeat].sub(total_raw_data[\"S&P500\"], axis=0))#.append(total_raw_data.iloc[:,-nfeat:])\n",
    "\n",
    "# recreate the dataframe\n",
    "total_raw_data_dt = pd.concat([detrende_stocks, total_raw_data.iloc[:,-nfeat:]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c8c2c-d2c8-47af-bbd1-5a569a2a08b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualizing the detrending\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 7))\n",
    "\n",
    "ax[0].plot(total_raw_data[\"NEE.N\"], alpha=0.5)\n",
    "ax[0].plot(total_raw_data_dt[\"NEE.N\"])\n",
    "ax[0].plot(total_raw_data[\"S&P500\"], alpha=0.5)\n",
    "\n",
    "ax[1].plot(total_raw_data[\"NEE.N\"].cumsum(), label=\"NEE.N\", alpha=0.5)\n",
    "ax[1].plot(total_raw_data_dt[\"NEE.N\"].cumsum(), label=\"NEE.N Detrended\")\n",
    "ax[1].plot(total_raw_data[\"S&P500\"].cumsum(), label=\"S&P500\", alpha=0.5)\n",
    "\n",
    "fig.suptitle(\"Stock vs. S&P vs. detrended stock\", fontsize=16)\n",
    "ax[0].set_ylabel(\"5d Return\")\n",
    "ax[0].spines[\"bottom\"].set_visible(False)\n",
    "ax[0].tick_params(axis='x', which='both', labelbottom=False)\n",
    "ax[0].xaxis.grid(True)\n",
    "\n",
    "ax[1].set_ylabel(\"Cumulative 5d Return\")\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].spines[\"top\"].set_visible(False)\n",
    "    ax[i].spines[\"right\"].set_visible(False)\n",
    "    ax[i].xaxis.grid(True)  # set zorder to 0 to show grid lines below tick marks\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# not this is not really a correct time series because it cumulates 5day returns. but for sake of visualizing its ok. \n",
    "# we're not using the cumulative time series anyways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c92260-fd03-459f-9777-888a84cf5491",
   "metadata": {
    "tags": []
   },
   "source": [
    "## creating usable features\n",
    "the idea is to calculate linear sensitivities of each stock towards the feature. for this, we first normalize all features (z-scores).\n",
    "Second we calculate the slopes of linear regressions (betas). Those betas will be of similar scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5925415-c73d-4081-a532-e96cbdd78cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, start_date, mid_date, end_date, feature_pos):\n",
    "    first_part = data.loc[start_date:mid_date]\n",
    "    second_part = data.loc[mid_date:end_date]\n",
    "    second_part_ex_feats = data.loc[mid_date:end_date].iloc[:,:-feature_pos]\n",
    "\n",
    "    return first_part, second_part[1:], second_part_ex_feats[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29397cf1-c966-47c4-ac69-6785d2c4c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifying_data, test_data, test_data_ex_feats = data_split(total_raw_data_dt, \"2020-01-08\", \"2022-12-31\", \"2023-3-31\", nfeat)\n",
    "\n",
    "# for later:\n",
    "corr_matrix_train = classifying_data.iloc[:, :-nfeat].corr()\n",
    "corr_matrix_test = test_data_ex_feats.corr()\n",
    "\n",
    "var_covar_matrix_train = np.cov(classifying_data.iloc[:, :-df_raw_features_slice_chg.shape[1]], rowvar=False)\n",
    "var_covar_matrix_train = np.cov(test_data_ex_feats, rowvar=False)\n",
    "\n",
    "# save matrices to harddrive\n",
    "file_name = 'total_raw_data{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "total_raw_data.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e5079-f018-4ac8-afc1-2ed02c2d21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last `nfeat` columns of your dataframe\n",
    "cols_to_standardize = classifying_data.columns[-nfeat:]\n",
    "standardized_features = classifying_data[cols_to_standardize].apply(zscore)\n",
    "stocks = classifying_data.iloc[:,:-nfeat]\n",
    "classifying_data = pd.concat([stocks,standardized_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db175bc-2f22-4b05-af91-d66d3140119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = classifying_data.iloc[:, :-df_raw_features_slice_chg.shape[1]]\n",
    "variables = classifying_data.iloc[:, -df_raw_features_slice_chg.shape[1]:]\n",
    "# Create an empty dataframe to store the beta coefficients\n",
    "betas = pd.DataFrame(index = stocks.columns, \n",
    "                     columns=variables.columns)\n",
    "correls = pd.DataFrame(index = stocks.columns, \n",
    "                     columns=variables.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a0085-220c-49ad-820e-5fb00b4049d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the columns of df1 and df2 and calculate the beta coefficients\n",
    "for col1 in stocks.columns:\n",
    "    for col2 in variables.columns:\n",
    "        beta, _, _, _, _ = linregress(variables[col2], stocks[col1])\n",
    "        betas.loc[col1, col2] = beta\n",
    "        \n",
    "# Loop through the columns of df1 and df2 and calculate the correlation coefficients\n",
    "for col1 in stocks.columns:\n",
    "    for col2 in variables.columns:\n",
    "        corr = variables[col2].corr(stocks[col1])\n",
    "        correls.loc[col1, col2] = corr\n",
    "\n",
    "# raw beta scaling        \n",
    "betas_std = np.std(betas.values, axis=None)\n",
    "betas_mean = np.mean(betas.values, axis=None)\n",
    "\n",
    "correls_std = np.std(correls.values, axis=None)\n",
    "correls_mean = np.mean(correls.values, axis=None)\n",
    "\n",
    "scaled_betas = (betas-betas_mean)/betas_std\n",
    "scaled_correls = (correls-correls_mean)/correls_std\n",
    "\n",
    "# merge scaled idiosyncratic features to scaled macro sensitivities\n",
    "scaled_betas = scaled_betas.join(idio_feats, how = \"inner\")\n",
    "scaled_correls = scaled_correls.join(idio_feats, how = \"inner\")\n",
    "\n",
    "# calculate the percentile ranks of the values in betas and correls and store it in rank_betas and rank_correls dataframes\n",
    "ranked_betas = scaled_betas.rank(pct=True)\n",
    "ranked_correls = scaled_correls.rank(pct=True)\n",
    "\n",
    "# combined values\n",
    "mean_scaled = pd.DataFrame(index=scaled_betas.index, columns=scaled_betas.columns)\n",
    "mean_ranked = pd.DataFrame(index=scaled_betas.index, columns=scaled_betas.columns)\n",
    "# calculate the mean of the scaled values\n",
    "mean_scaled = (scaled_betas + scaled_correls) / 2\n",
    "# calculate the mean of the percentile ranks\n",
    "mean_ranked = (ranked_betas + ranked_correls) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ecfb15-39a6-40c6-baab-d63c50b86544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a blue color palette\n",
    "colors = sns.color_palette(\"Blues\")\n",
    "\n",
    "# Create a figure with 5 subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10), sharex=True)\n",
    "\n",
    "# Set the style for all subplots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a boxplot for each column of betas and add it to a subplot\n",
    "\n",
    "\n",
    "sns.boxplot(data=scaled_betas, ax=axs[0, 0], palette=colors)\n",
    "axs[0, 0].set_title('Boxplot of Scaled Betas')\n",
    "axs[0, 0].set_ylabel('Scaled Beta')\n",
    "\n",
    "sns.boxplot(data=scaled_correls, ax=axs[0, 1], palette=colors)\n",
    "axs[0, 1].set_title('Boxplot of Scaled Correls')\n",
    "axs[0, 1].set_ylabel('Rank Beta')\n",
    "\n",
    "sns.boxplot(data=mean_scaled, ax=axs[0, 2], palette=colors)\n",
    "axs[0, 2].set_title('Boxplot of Mean_Scaled')\n",
    "axs[0, 2].set_ylabel('Mean')\n",
    "\n",
    "sns.boxplot(data=ranked_betas, ax=axs[1, 0], palette=colors)\n",
    "axs[1, 0].set_title('Boxplot of Ranked Betas')\n",
    "axs[1, 0].set_ylabel('Rank Correlation')\n",
    "\n",
    "sns.boxplot(data=ranked_correls, ax=axs[1, 1], palette=colors)\n",
    "axs[1, 1].set_title('Boxplot of Ranked Correls')\n",
    "axs[1, 1].set_ylabel('Rank Correlation')\n",
    "\n",
    "sns.boxplot(data=mean_ranked, ax=axs[1, 2], palette=colors)\n",
    "axs[1, 2].set_title('Boxplot of Mean_Ranked')\n",
    "axs[1, 2].set_ylabel('Mean')\n",
    "\n",
    "# Rotate x-axis labels by 90 degrees for all subplots\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9981d2-ec04-4736-a6d6-fc769d2fbd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a blue color palette\n",
    "colors = sns.color_palette(\"Blues\")\n",
    "\n",
    "# Create a figure with 5 subplots\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10), sharex=True)\n",
    "\n",
    "# Set the style for all subplots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a boxplot for each column of betas and add it to a subplot\n",
    "sns.boxplot(data=betas, ax=axs[0, 0], palette=colors)\n",
    "axs[0, 0].set_title('Boxplot of Raw Betas')\n",
    "axs[0, 0].set_ylabel('Beta')\n",
    "\n",
    "sns.boxplot(data=scaled_betas, ax=axs[0, 1], palette=colors)\n",
    "axs[0, 1].set_title('Boxplot of Scaled Betas')\n",
    "axs[0, 1].set_ylabel('Z-Score')\n",
    "\n",
    "sns.boxplot(data=scaled_correls, ax=axs[0, 2], palette=colors)\n",
    "axs[0, 2].set_title('Boxplot of Scaled Correlations')\n",
    "axs[0, 2].set_ylabel('Z-Score')\n",
    "\n",
    "sns.boxplot(data=mean_scaled, ax=axs[0, 3], palette=colors)\n",
    "axs[0, 3].set_title('Boxplot of Mean_Scaled')\n",
    "axs[0, 3].set_ylabel('avg. Z-Score')\n",
    "\n",
    "sns.boxplot(data=correls, ax=axs[1, 0], palette=colors)\n",
    "axs[1, 0].set_title('Boxplot of Raw Correlations')\n",
    "axs[1, 0].set_ylabel('Correlation')\n",
    "\n",
    "sns.boxplot(data=ranked_betas, ax=axs[1, 1], palette=colors)\n",
    "axs[1, 1].set_title('Boxplot of Ranked Betas')\n",
    "axs[1, 1].set_ylabel('Percentile Rank')\n",
    "\n",
    "sns.boxplot(data=ranked_correls, ax=axs[1, 2], palette=colors)\n",
    "axs[1, 2].set_title('Boxplot of Ranked Correlations')\n",
    "axs[1, 2].set_ylabel('Percentile Rank')\n",
    "\n",
    "sns.boxplot(data=mean_ranked, ax=axs[1, 3], palette=colors)\n",
    "axs[1, 3].set_title('Boxplot of Mean_Ranked')\n",
    "axs[1, 3].set_ylabel('avg. Percentile Rank')\n",
    "\n",
    "# Rotate x-axis labels by 90 degrees for all subplots\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fa41e-c1b5-430b-9735-34849f6437b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685bcbc-dc1c-4123-9122-09a22a387b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d721caa-5f25-4513-85e1-b680ee954442",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ignore rest from here on down, legacy code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077f9e5-5ca8-4c04-aa22-052c1b31bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_of_dfs = [scaled_betas, ranked_betas, scaled_correls, ranked_correls, mean_scaled, mean_ranked]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943589c-fc55-40e7-96ac-3dfc6fa067c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb67dc-68cc-4119-9cd6-d5733fce2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting cross correls per cluster\n",
    "\n",
    "def map_stocks_to_clusters(stock_labels): # stock_labels is an array\n",
    "    # Create an empty dictionary to store the mapping between stock labels and their respective classes.\n",
    "    class_stock_mapping = {}\n",
    "    # Loop through each stock label and its index in the input list.\n",
    "    for i, stock_label in enumerate(stock_labels):\n",
    "        # If the stock label is not already in the class_stock_mapping dictionary, add it as a key with an empty list value.\n",
    "        if stock_label not in class_stock_mapping:\n",
    "            class_stock_mapping[stock_label] = []\n",
    "        # Append the index of the current stock label to its corresponding class list.\n",
    "        class_stock_mapping[stock_label].append(i+1)\n",
    "    # Sort the dictionary by the keys in ascending order.\n",
    "    class_stock_mapping = dict(sorted(class_stock_mapping.items()))\n",
    "    # Return the final dictionary that maps stock labels to their respective classes.\n",
    "    return class_stock_mapping\n",
    "\n",
    "def calculate_avg_interclass_corr(class_stock_mapping, stock_returns): # class_stock_mapping from the previous function, stock_returns is the return data\n",
    "    # create an empty dictionary to store the dataframes and correlation matrices\n",
    "    Cluster = {}\n",
    "    # create an empty list to store the average interclass correlations\n",
    "    avg_Cluster_Correls = []\n",
    "    # loop over the stock_to_class dictionary using the enumerate function to get the index and the key-value pairs\n",
    "    for i, (key, value) in enumerate(class_stock_mapping.items()):\n",
    "        # concatenate the dataframes using a list comprehension and store the result in the H_Cluster dictionary\n",
    "        # key is used as the dictionary key to store the result\n",
    "        Cluster[key] = pd.concat([stock_returns.iloc[:, idx-1:idx] for idx in value], axis=1)\n",
    "        # calculate the correlation matrix for the current dataframe and store it in a variable\n",
    "        corr_mtrx = Cluster[key].corr()\n",
    "        # calculate the interclass correlation excluding selfcorrelations \n",
    "        interclass_corr = (corr_mtrx.mean().mean() * len(corr_mtrx)**2 - len(corr_mtrx)) / (len(corr_mtrx)**2 - len(corr_mtrx))\n",
    "        # append the average interclass correlation to the avg_H_Cluster_Correls list\n",
    "        avg_Cluster_Correls.append(round(interclass_corr, 3))\n",
    "    # return the resulting list of average interclass correlations\n",
    "    return avg_Cluster_Correls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184803b0-96d3-4e81-8fd5-399fd28f7e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Level_1 = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(sector_classes_code.iloc[:, 1].to_numpy()), stock_returns = test_data_ex_feats)\n",
    "Level_1 = [x for x in Level_1 if not math.isnan(x)]\n",
    "\n",
    "Level_2 = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(sector_classes_code.iloc[:, 2].to_numpy()), stock_returns = test_data_ex_feats)\n",
    "Level_2 = [x for x in Level_2 if not math.isnan(x)]\n",
    "\n",
    "Level_3 = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(sector_classes_code.iloc[:, 3].to_numpy()), stock_returns = test_data_ex_feats)\n",
    "Level_3 = [x for x in Level_3 if not math.isnan(x)]\n",
    "\n",
    "Level_4 = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(sector_classes_code.iloc[:, 4].to_numpy()), stock_returns = test_data_ex_feats)\n",
    "Level_4 = [x for x in Level_4 if not math.isnan(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3c33d-3646-445b-8a03-e1f3cf0746e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_clust_results = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_kmean_correl), stock_returns = test_data_ex_feats)\n",
    "h_clust_results = [x for x in h_clust_results if not math.isnan(x)]\n",
    "\n",
    "\n",
    "h_clust_results_100 = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_3d), stock_returns = test_data_ex_feats)\n",
    "h_clust_results_100 = [x for x in h_clust_results_100 if not math.isnan(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19f962-6edf-4b46-a4d6-3ebfba11e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a figure with two subplots arranged side-by-side\n",
    "fig, ax = plt.subplots(1, 6, figsize=(15, 5))\n",
    "\n",
    "ax[0].boxplot(Level_1)\n",
    "ax[0].set_title('GICS Level 1')\n",
    "ax[0].set_ylabel('Average Interclass Correlation')\n",
    "\n",
    "ax[1].boxplot(Level_2)\n",
    "ax[1].set_title('GICS Level 2')\n",
    "\n",
    "ax[2].boxplot(Level_3)\n",
    "ax[2].set_title('GICS Level 3')\n",
    "\n",
    "ax[3].boxplot(Level_4)\n",
    "ax[3].set_title('GICS Level 4')\n",
    "\n",
    "ax[4].boxplot(h_clust_results)\n",
    "ax[4].set_title('Hierachical Clustering vs. Level 1')\n",
    "\n",
    "ax[5].boxplot(h_clust_results_100)\n",
    "ax[5].set_title('Hierachical Clustering vs. Level 4')\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i].set_ylim([-1,1])\n",
    "    ax[i].set_xticklabels([])  # remove x-labels\n",
    "    if i == 5:\n",
    "        break\n",
    "    else:\n",
    "        ax[i+1].set_yticklabels([])  # remove x-labels\n",
    "\n",
    "    \n",
    "    \n",
    "# Add a shared y-axis label and adjust spacing between subplots\n",
    "fig.text(0.5, 0.04, 'Average Interclass Correlation', ha='center', fontsize=14)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c651945-15d1-44cf-851f-5c1abd634c0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9f53-d323-4b71-a8e9-137d13d927c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_stats = corr_matrix_train.describe()\n",
    "corr_stats_2 = corr_matrix_test.describe()\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot the boxplot in the first subplot\n",
    "sns.violinplot(ax=axes[0], data=corr_stats.iloc[1:-1].transpose(), color='green')\n",
    "axes[0].set_title('train')\n",
    "axes[0].set_ylim(-1, 1)\n",
    "axes[0].set_ylabel('Beta')\n",
    "\n",
    "# Plot the violinplot in the second subplot\n",
    "sns.violinplot(ax=axes[1], data=corr_stats_2.iloc[1:-1].transpose(), color='green')\n",
    "axes[1].set_title('test')\n",
    "axes[1].set_ylim(-1, 1)\n",
    "axes[1].set_ylabel('Beta')\n",
    "\n",
    "# Adjust the layout and show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9f016-f3ca-4744-989a-b9d21f2a914c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## unnecessary clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dcc30-d51a-4a0f-8d0d-66fe4e20b7fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## hirachical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43e0be-398d-405d-818a-2097da4e0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c06f92-8ae6-46f2-af35-2339165c8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns of the DataFrame to float64\n",
    "betas = scaled_betas.astype('float64')\n",
    "\n",
    "# Calculate the distance matrix based on the features\n",
    "distance_matrix = pdist(betas.values, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical clustering using the distance matrix\n",
    "clusters = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Set the number of colorized clusters\n",
    "num_color_clusters = 10\n",
    "\n",
    "# Create a dendrogram with the specified number of clusters colorized\n",
    "plt.figure(figsize=(12, 6))\n",
    "color_threshold = clusters[-num_color_clusters, 2]\n",
    "dendrogram(clusters, labels=betas.index, leaf_rotation=90, color_threshold=color_threshold)\n",
    "plt.axhline(y=color_threshold, c='gray', lw=1, linestyle='dashed')\n",
    "\n",
    "\n",
    "plt.title(f\"Hierarchical Clustering Dendrogram with First {num_color_clusters+1} Clusters Colorized\")\n",
    "#plt.gca().set_ylim([0.01, 1.5])\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Stocks\")\n",
    "plt.ylabel(\"Euclidean Distances\")\n",
    "plt.show()\n",
    "\n",
    "# get array with clusters\n",
    "h_clust_labes = fcluster(clusters, num_color_clusters, criterion='maxclust')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e731a-fbe1-4090-b9e6-8fbe37eb3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns of the DataFrame to float64\n",
    "betas = scaled_betas.astype('float64')\n",
    "\n",
    "# Calculate the distance matrix based on the features\n",
    "distance_matrix = pdist(betas.values, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical clustering using the distance matrix\n",
    "clusters = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Set the number of colorized clusters\n",
    "num_color_clusters = 100\n",
    "\n",
    "# Create a dendrogram with the specified number of clusters colorized\n",
    "plt.figure(figsize=(12, 6))\n",
    "color_threshold = clusters[-num_color_clusters, 2]\n",
    "dendrogram(clusters, labels=betas.index, leaf_rotation=90, color_threshold=color_threshold)\n",
    "plt.axhline(y=color_threshold, c='gray', lw=1, linestyle='dashed')\n",
    "\n",
    "\n",
    "plt.title(f\"Hierarchical Clustering Dendrogram with First {num_color_clusters+1} Clusters Colorized\")\n",
    "#plt.gca().set_ylim([0.01, 1.5])\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Stocks\")\n",
    "plt.ylabel(\"Euclidean Distances\")\n",
    "plt.show()\n",
    "\n",
    "# get array with clusters\n",
    "h_clust_labes_100 = fcluster(clusters, num_color_clusters, criterion='maxclust')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32406b23-5d63-4d4e-8ca3-d68b9e1b8a79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PCA & k-means on betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4c96f-eb55-4c6c-b74c-cd2bb7f69f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf43a5f-0c50-44eb-8857-5cef503ee5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance-covariance matrix\n",
    "\n",
    "# Perform PCA to reduce dimensionality of the data\n",
    "pca = PCA(n_components=28)\n",
    "pca.fit(betas)\n",
    "betas_pca = pca.transform(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dc0b5-f4fc-469f-b24f-ebb5ade18c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7966793-6fa7-48a7-9b26-cfc07d459053",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ceaebe-2d49-4bdd-9d16-a2d1fae02ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=9)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c3c1e-6b1c-4ca6-84b2-189eb046cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an elbow plot to determine the optimal number of principal components\n",
    "pca_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(np.arange(1, len(pca_var)+1), pca_var)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.xticks(np.arange(1, len(pca_var)+1)) # Set x-ticks to match number of principal components\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.title('Elbow Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11372c93-ceb7-44f9-80ba-18b6a331c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca_2d = PCA(n_components=3)\n",
    "pca_3d = PCA(n_components=8)\n",
    "\n",
    "pca_2d.fit(betas.iloc[:,:15])\n",
    "pca_3d.fit(betas.iloc[:,:15])\n",
    "\n",
    "betas_pca_2d = pca_2d.transform(betas.iloc[:,:15])\n",
    "betas_pca_3d = pca_3d.transform(betas.iloc[:,:15])\n",
    "\n",
    "# Define a custom colormap\n",
    "custom_cmap = plt.cm.get_cmap('viridis', 10)\n",
    "\n",
    "# Perform k-means clustering on the principal components\n",
    "kmeans_2d = KMeans(n_clusters=10)\n",
    "kmeans_3d = KMeans(n_clusters=10)\n",
    "\n",
    "kmeans_2d.fit(betas_pca_2d)\n",
    "kmeans_3d.fit(betas_pca_3d)\n",
    "\n",
    "labels_2d = kmeans_2d.labels_\n",
    "labels_3d = kmeans_3d.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d4532-f453-497e-a54d-bee97fec3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of plots with both the 2D and 3D scatter plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].scatter(betas_pca_2d[:,0], betas_pca_2d[:,1], c=kmeans_2d.labels_, cmap=custom_cmap)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].set_title('2D Scatter Plot')\n",
    "\n",
    "ax[1] = fig.add_subplot(122, projection='3d')\n",
    "ax[1].scatter(betas_pca[:,0], betas_pca[:,1], betas_pca[:,2], c=kmeans_3d.labels_, cmap=custom_cmap)\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].set_zlabel('PC3')\n",
    "ax[1].set_title('3D Scatter Plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da6bf0-94e8-4d94-9c9d-aba9c56c7722",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BM Clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c36c3-22c7-40cf-b70e-399d480e01a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## k-means on correlmatrix alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f5410-2e13-4a64-9d02-b0aa5fc17b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b62261-45fa-4653-84ec-db7822fc27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [corr_matrix_train, corr_matrix_test]\n",
    "\n",
    "# loop through cluster depths and dataframes\n",
    "results_BM = pd.DataFrame(columns=[\"Stock\"])\n",
    "\n",
    "cluster_depths = [11, 25, 67, 126]\n",
    "\n",
    "for n_clusters in cluster_depths:\n",
    "\n",
    "    for df_name, df in zip([\"corr_matrix_train\", \"corr_matrix_test\"], df_list):\n",
    "        print(f\"Working on {df_name} with depth {n_clusters}...\")\n",
    "        \n",
    "        cluster_algos = {\n",
    "            \"KMeans\": KMeans(n_clusters=n_clusters, max_iter=5000, n_init=1000),\n",
    "            \"GMM\": GaussianMixture(n_components=n_clusters, random_state=42),\n",
    "            \"Hierarchical\": AgglomerativeClustering(n_clusters=n_clusters),\n",
    "            \"Birch\": Birch(threshold=0.5, n_clusters=n_clusters)\n",
    "        }\n",
    "        df_result = pd.DataFrame({\"Stock\": df.iloc[:, 0]})\n",
    "        df_data = df.iloc[:, 1:]\n",
    "        \n",
    "        # loop through clustering algorithms\n",
    "        for algo_name, algo in cluster_algos.items():\n",
    "            algo_results = algo.fit_predict(df_data)\n",
    "            df_result[f\"{algo_name}_{n_clusters}_{df_name}\"] = algo_results\n",
    "        \n",
    "        # check for missing or duplicated rows in df_result\n",
    "        if df_result[\"Stock\"].duplicated().any() or df_result[\"Stock\"].isna().any():\n",
    "            print(f\"Warning: {df_name} with depth {n_clusters} contains missing or duplicated rows.\")\n",
    "        \n",
    "        # merge results with df_result\n",
    "        results_BM = pd.merge(results, df_result, on=\"Stock\", how=\"outer\")\n",
    "        results_BM.to_excel(\"results_after_algos.xlsx\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4450bf-9c9e-4397-a890-e72e1827f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "train_kmeans_11_a = KMeans(n_clusters=11, max_iter=1000, n_init=3000).fit(corr_matrix_train)\n",
    "train_kmeans_11_b = KMeans(n_clusters=11, max_iter=1000, n_init=3000).fit(corr_matrix_train)\n",
    "\n",
    "# Get the cluster labels for each model\n",
    "labels_a = train_kmeans_11_a.labels_\n",
    "labels_b = train_kmeans_11_b.labels_\n",
    "\n",
    "# Calculate the Adjusted Rand Index between the two clusterings\n",
    "ari = adjusted_rand_score(labels_a, labels_b)\n",
    "print(f\"The Adjusted Rand Index between the two clusterings is {ari}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5806f-3338-460d-9404-b773184d1e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e68af-1add-4fd0-b9dd-99b070193648",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[:,:-15].corr().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd5068-4f2f-42d5-80a1-c2b31c25cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "\n",
    "\n",
    "#list of all crosscorrelations\n",
    "crosscorr_matrix = test_data.iloc[:,:-15].corr()\n",
    "lower_elements = np.tril(crosscorr_matrix, k=-1).flatten()\n",
    "crosscorrels = pd.Series([element for element in lower_elements if element != 0 and element != 1])\n",
    "crosscorrels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb7b2c-1487-4669-a813-02f3a3c7cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with two subplots arranged side-by-side\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "ax[0].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(stock_labels), stock_returns = test_data))\n",
    "ax[0].set_title('H_Clust')\n",
    "ax[0].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "ax[1].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_2d), stock_returns = test_data))\n",
    "ax[1].set_title('PCA_KMEANS_2d')\n",
    "ax[1].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "ax[2].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_3d), stock_returns = test_data))\n",
    "ax[2].set_title('PCA_KMEANS_3d')\n",
    "ax[2].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "ax[3].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_kmean_correl), stock_returns = test_data))\n",
    "ax[3].set_title('KMEANS_correl')\n",
    "ax[3].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "# Create a boxplot for the third dataset\n",
    "ax[4].boxplot(crosscorrels)\n",
    "ax[4].set_title('All Crosscorrels')\n",
    "ax[4].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "# Add a shared y-axis label and adjust spacing between subplots\n",
    "fig.text(0.5, 0.04, 'Average Interclass Correlation', ha='center', fontsize=14)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317446f-9039-460e-b4cb-b9550f6e234a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## tests   klappt noch nicht wirklich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d11abf-4e5f-47cc-8b71-3b97732dddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb61031-fdf8-47c3-9153-dd7842c52eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca_feat_1 = PCA(n_components=3)\n",
    "pca_feat_2 = PCA(n_components=3)\n",
    "pca_feat_3 = PCA(n_components=3)\n",
    "pca_feat_4 = PCA(n_components=3)\n",
    "pca_feat_5 = PCA(n_components=3)\n",
    "pca_feat_6 = PCA(n_components=3)\n",
    "\n",
    "pca_feat_1.fit(feature_list_of_dfs[0])\n",
    "pca_feat_2.fit(feature_list_of_dfs[1])\n",
    "pca_feat_3.fit(feature_list_of_dfs[2])\n",
    "pca_feat_4.fit(feature_list_of_dfs[3])\n",
    "pca_feat_5.fit(feature_list_of_dfs[4])\n",
    "pca_feat_6.fit(feature_list_of_dfs[5])\n",
    "\n",
    "feat_pca_1 = pca_feat_1.transform(feature_list_of_dfs[0])\n",
    "feat_pca_2 = pca_feat_2.transform(feature_list_of_dfs[1])\n",
    "feat_pca_3 = pca_feat_3.transform(feature_list_of_dfs[2])\n",
    "feat_pca_4 = pca_feat_4.transform(feature_list_of_dfs[3])\n",
    "feat_pca_5 = pca_feat_5.transform(feature_list_of_dfs[4])\n",
    "feat_pca_6 = pca_feat_6.transform(feature_list_of_dfs[5])\n",
    "\n",
    "# Define a custom colormap\n",
    "custom_cmap = plt.cm.get_cmap('viridis', 10)\n",
    "\n",
    "# Perform k-means clustering on the principal components\n",
    "kmeans_feat_1 = KMeans(n_clusters=len(Level_4))\n",
    "kmeans_feat_2 = KMeans(n_clusters=10)\n",
    "kmeans_feat_3 = KMeans(n_clusters=10)\n",
    "kmeans_feat_4 = KMeans(n_clusters=10)\n",
    "kmeans_feat_5 = KMeans(n_clusters=10)\n",
    "kmeans_feat_6 = KMeans(n_clusters=10)\n",
    "\n",
    "kmeans_feat_1.fit(feat_pca_1)\n",
    "kmeans_feat_2.fit(feat_pca_2)\n",
    "kmeans_feat_3.fit(feat_pca_3)\n",
    "kmeans_feat_4.fit(feat_pca_4)\n",
    "kmeans_feat_5.fit(feat_pca_5)\n",
    "kmeans_feat_6.fit(feat_pca_6)\n",
    "\n",
    "labels_feat_1 = kmeans_feat_1.labels_\n",
    "labels_feat_2 = kmeans_feat_2.labels_\n",
    "labels_feat_3 = kmeans_feat_3.labels_\n",
    "labels_feat_4 = kmeans_feat_4.labels_\n",
    "labels_feat_5 = kmeans_feat_5.labels_\n",
    "labels_feat_6 = kmeans_feat_6.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e16ca-ac24-4e91-8afe-257fa7914a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a figure with two subplots arranged side-by-side\n",
    "fig, ax = plt.subplots(1, 6, figsize=(15, 5))\n",
    "\n",
    "ax[0].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_1), stock_returns = test_data))\n",
    "ax[0].set_title('Betas')\n",
    "ax[0].set_ylabel('Average Interclass Correlation')\n",
    "\n",
    "ax[1].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_2), stock_returns = test_data))\n",
    "ax[1].set_title('Centered Betas')\n",
    "\n",
    "ax[2].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_3), stock_returns = test_data))\n",
    "ax[2].set_title('Rank Betas')\n",
    "\n",
    "ax[3].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_4), stock_returns = test_data))\n",
    "ax[3].set_title('Correls')\n",
    "\n",
    "ax[4].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_5), stock_returns = test_data))\n",
    "ax[4].set_title('Rank Correls')\n",
    "\n",
    "ax[5].boxplot(calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(labels_feat_6), stock_returns = test_data))\n",
    "ax[5].set_title('Mean Rank')\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i].set_ylim([0,0.75])\n",
    "    ax[i].set_xticklabels([])  # remove x-labels\n",
    "    if i == 5:\n",
    "        break\n",
    "    else:\n",
    "        ax[i+1].set_yticklabels([])  # remove x-labels\n",
    "\n",
    "    \n",
    "    \n",
    "# Add a shared y-axis label and adjust spacing between subplots\n",
    "fig.text(0.5, 0.04, 'Average Interclass Correlation', ha='center', fontsize=14)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc1615-a17a-4861-9a5e-0b51289af2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusterings = betas.iloc[:,:0]\n",
    "\n",
    "# Add the labels as a new column to the betas DataFrame\n",
    "Clusterings['H_Clust'] = stock_labels\n",
    "Clusterings['PCA_KMEANS_2d'] = labels_2d\n",
    "Clusterings['PCA_KMEANS_3d'] = labels_3d\n",
    "Clusterings['KMEANS_correl'] = labels_kmean_correl\n",
    "\n",
    "plt.plot(Clusterings.sort_values('H_Clust')['H_Clust'])\n",
    "plt.plot(Clusterings.sort_values('H_Clust')['PCA_KMEANS_2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d047776-afd3-4ffd-aebc-ab49f12da19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11c198-cb96-4c24-a437-f8d48bc2f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc738a-fc3e-40e9-89ec-f78f229c92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# compute the Adjusted Rand Index between all pairs of columns (i.e., clusterings)\n",
    "ari_scores = []\n",
    "for c1, c2 in combinations(Clusterings.columns, 2):\n",
    "    ari_score = adjusted_rand_score(Clusterings[c1], Clusterings[c2])\n",
    "    ari_scores.append([c1, c2, ari_score])\n",
    "\n",
    "# compute the Normalized Mutual Information between all pairs of columns (i.e., clusterings)\n",
    "nmi_scores = []\n",
    "for c1, c2 in combinations(Clusterings.columns, 2):\n",
    "    nmi_score = normalized_mutual_info_score(Clusterings[c1], Clusterings[c2])\n",
    "    nmi_scores.append([c1, c2, nmi_score])\n",
    "\n",
    "# create dataframes to store the ARI and NMI scores\n",
    "ari_df = pd.DataFrame(ari_scores, columns=['Clustering 1', 'Clustering 2', 'ARI'])\n",
    "nmi_df = pd.DataFrame(nmi_scores, columns=['Clustering 1', 'Clustering 2', 'NMI'])\n",
    "\n",
    "# create a pivot table to display the ARI and NMI scores as a heatmap\n",
    "ari_pivot = ari_df.pivot(index='Clustering 1', columns='Clustering 2', values='ARI')\n",
    "nmi_pivot = nmi_df.pivot(index='Clustering 1', columns='Clustering 2', values='NMI')\n",
    "\n",
    "# plot the ARI and NMI heatmaps side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.heatmap(ari_pivot, cmap='coolwarm', annot=True, fmt='.3f', vmin=-1, vmax=1, ax=ax[0])\n",
    "sns.heatmap(nmi_pivot, cmap='coolwarm', annot=True, fmt='.3f', vmin=0, vmax=1, ax=ax[1])\n",
    "ax[0].set_title('Adjusted Rand Index (ARI)')\n",
    "ax[1].set_title('Normalized Mutual Information (NMI)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2060e17-dd6a-4873-91a4-34c5c55496e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# compute the pairwise correlation between all pairs of columns (i.e., clusterings) using the ARI metric\n",
    "correlation_matrix = pd.DataFrame(index=Clusterings.columns, columns=Clusterings.columns)\n",
    "for i in range(len(Clusterings.columns)):\n",
    "    for j in range(len(Clusterings.columns)):\n",
    "        correlation_matrix.iloc[i, j] = adjusted_rand_score(Clusterings.iloc[:, i], Clusterings.iloc[:, j])\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "# plot a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, vmin=-1, vmax=1)\n",
    "plt.title('Similarity of Clustering Results')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a244a-8273-4744-ab0d-d858fe25a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clusters = np.arange(1, 11)  # Create an array of cluster labels from 1 to 10\n",
    "one_hot_vectors = np.eye(10)[clusters - 1]  # Apply one-hot encoding to the clusters\n",
    "clusters, one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebc51a-f643-4ba5-84ff-0a88108b7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(10)[Clusterings[\"H_Clust\"]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05786cfc-c337-4133-91e9-40a265496a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the Clusterings DataFrame\n",
    "\n",
    "# One-hot encode the cluster columns\n",
    "one_hot_clusters = pd.get_dummies(Clusterings)\n",
    "\n",
    "# Calculate the cosine similarity between all pairs of clusters\n",
    "similarity_matrix = cosine_similarity(one_hot_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de54f7f-00b4-4133-8895-17f32cae63ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6eab9-1ca7-46af-a39b-a8901c7c92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the file path to where you want to save the DataFrame\n",
    "desktop_path = '~/Desktop/ML2_assignment_5_project/'\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e001c-a61e-4cf6-8fd8-60524aa99100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all training data in one df\n",
    "file_name = 'time_series_train_data_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "classifying_data.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "# all test data in one df\n",
    "file_name = 'time_series_test_data_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "test_data.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "# raw data features \n",
    "file_name = 'time_series_df_raw_features_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "df_raw_features_slice.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "# raw data stocks \n",
    "file_name = 'time_series_df_raw_stocks_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "result_df_clean.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "## sector mapping names\n",
    "file_name = 'sector_classes_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "sector_classes.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "## sector mapping codes \n",
    "file_name = 'sector_classes_code_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "sector_classes_code.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94d492-abb6-4861-9867-b774f9c26001",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sector mapping codes \n",
    "file_name = 'sector_classes_code_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "sector_classes_code.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a0980-e49c-4a8e-a4c0-08caa3977cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stock time series\n",
    "file_name = 'final_stock_timeseries.csv'\n",
    "target.round(5).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e074b-5315-42e0-9750-2a1d3cdf6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## matrices\n",
    "feature_list_of_dfs = [scaled_betas, ranked_betas, scaled_correls, ranked_correls, mean_scaled, mean_ranked]\n",
    "\n",
    "file_name = 'final_scaled_betas.csv'\n",
    "feature_list_of_dfs[0].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "file_name = 'final_ranked_betas.csv'\n",
    "feature_list_of_dfs[1].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "file_name = 'final_scaled_correls.csv'\n",
    "feature_list_of_dfs[2].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "file_name = 'final_rank_correls.csv'\n",
    "feature_list_of_dfs[3].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "file_name = 'final_mean_scaled.csv'\n",
    "feature_list_of_dfs[4].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n",
    "\n",
    "file_name = 'final_mean_ranked.csv'\n",
    "feature_list_of_dfs[5].round(3).to_csv(desktop_path + file_name, index_label='timestamp', date_format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae43ae4-9189-41a2-b551-5f48eb1d0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_betas.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5344b-fca8-439a-8216-3dfb15a02c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1\n",
    "data2\n",
    "data3\n",
    "data4\n",
    "data5\n",
    "data6\n",
    "\n",
    "data1_PCA\n",
    "data2_PCA\n",
    "data3_PCA\n",
    "data4_PCA\n",
    "data5_PCA\n",
    "data6_PCA\n",
    "\n",
    "cluster_depth = [11,25,67,126]\n",
    "data_list = [data1, data2, ..., data6_PCA]\n",
    "\n",
    "result_df = pandas\n",
    "for cd in cluster_depth:\n",
    "    for data in data_ist:\n",
    "        algo1(data, cd)\n",
    "        #add return to dataframe\n",
    "        \n",
    "\n",
    "\n",
    "def algo1(data, cluster_depth):\n",
    "    return cluster_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed02b47-2ed0-4b90-a427-e09488e8c7b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## resulst post clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096c9df-fe49-4338-bdb4-a1b2e41da8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from desktop\n",
    "path = r\"~/Desktop/ML2_assignment_5_project/results_after_algos.xlsx\"\n",
    "results_after_algos = pd.read_excel(path)\n",
    "results_after_algos = results_after_algos.set_index(results_after_algos.columns[0])\n",
    "\n",
    "path = r\"~/Desktop/ML2_assignment_5_project/results_after_algos_order.xlsx\"\n",
    "model_descr = pd.read_excel(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bc617-d744-4a06-949a-970572a749d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = model_descr.iloc[:,0:1]\n",
    "model_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a5309-fb1b-452c-810c-ac3a4b5dfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in new_order.values.tolist() for item in sublist]\n",
    "results_after_algos = results_after_algos.loc[:, flat_list]\n",
    "results_after_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f1ae4-1c1b-4b4b-83df-e2807d2e58a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustermap = sns.clustermap((results_after_algos), cmap='coolwarm', method='ward')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df194a1-23d6-4dfa-b3e4-cd9830c45613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_models = results_after_algos.shape[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    for i in range(n_models):\n",
    "        temp = results_after_algos.iloc[:,i:i+1].values.flatten()\n",
    "        temp_res = calculate_avg_interclass_corr(class_stock_mapping = map_stocks_to_clusters(temp), stock_returns = test_data_ex_feats)\n",
    "        temp_res = [x for x in temp_res if not math.isnan(x)]\n",
    "        results.append(temp_res)\n",
    "        \n",
    "result_means = [np.mean(result) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c66572-b7e0-4243-99ea-ca5a3548e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = model_descr\n",
    "final_results = final_results.assign(new_col=result_means).rename(columns={'new_col': 'avg. cluster performance'})\n",
    "final_results = final_results.sort_values(by='avg. cluster performance')\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec2b8b-5117-4e2a-8f9f-c1afcb49f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_results[\"avg. cluster performance\"].values\n",
    "# Set the plot style\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.bar(final_results['full_name'], final_results['avg. cluster performance'])\n",
    "\n",
    "# Add axis labels and a title\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Average Cluster Performance')\n",
    "ax.set_title('Results of Clustering Algorithms')\n",
    "\n",
    "# Rotate the x-axis tick labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a20bc-78f2-4bed-a0f0-0ab82422658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# create a pivot table with the parameters as columns and rows and the score as values\n",
    "pivot_table = final_results.pivot_table(index=['type', 'depth'], columns=['inputs', 'dims'], values='avg. cluster performance')\n",
    "\n",
    "# create a heatmap\n",
    "sns.heatmap(pivot_table, cmap='coolwarm')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7faf14-4c2b-46d9-a3cb-ffba038ec75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a pivot table with the parameters as columns and rows and the score as values\n",
    "pivot_table = final_results.pivot_table(index=['type', 'depth'], columns=['inputs', 'dims'], values='avg. cluster performance')\n",
    "\n",
    "# add the row averages as a new column\n",
    "pivot_table.loc[:, ('row_avg', '')] = pivot_table.mean(axis=1)\n",
    "\n",
    "# add the column averages as a new row\n",
    "pivot_table.loc[('', 'col_avg'), :] = pivot_table.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# create a heatmap\n",
    "sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='.3f')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
